{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom glob import glob\nfrom tqdm import tqdm\nfrom multiprocessing import Pool, cpu_count\n\ndef load_all_csv():\n\n    \"\"\"\n    Load all the features csv files, keep the desired columns, and sort by bookingID and second.\n    \"\"\"\n    \n    paths = glob('../input/safety/safety/features/*.csv')\n    \n    features = pd.read_csv(paths[0])\n    \n    for path in tqdm(paths[1:]):\n        new_df = pd.read_csv(path)\n        features = pd.concat([features, new_df])\n\n    # check that the features are present and in sequence\n    try:\n        features = features[[   \n                                'bookingID',\n                                'Accuracy',\n                                'Bearing',\n                                'acceleration_x',\n                                'acceleration_y',\n                                'acceleration_z',\n                                'gyro_x',\n                                'gyro_y',\n                                'gyro_z',\n                                'second',\n                                'Speed'\n                            ]]\n    except KeyError:\n        print('One or more feature columns not found. Please ensure only csv files with the exact columns are in the directory')\n    \n    features.drop(columns=['gyro_x','gyro_y','gyro_z'], inplace=True)\n    features.sort_values(by=['bookingID','second'], inplace=True)\n    features.set_index(['bookingID','second'], inplace=True)\n    \n    if not (~features.isna()).all().all():\n        # interpolate dataframe if there are missing numerical data\n        features.interpolate(inplace=True)\n        \n    return features\n\ndef get_booking_details(dataframe):\n\n    \"\"\"\n    Utility function to obtain the booking start index, end index and total length (rows of data associated with the booking), for optimized feature engineering.\n    \"\"\"\n    bking_index = dataframe.index.get_level_values(0).unique()\n    bking_index_start = dataframe[['Accuracy']].groupby('bookingID').count().cumsum().shift(1).values.ravel()\n    bking_index_start[0] = 0\n    bking_index_start = bking_index_start.astype(np.int64)\n    bking_index_end = dataframe[['Accuracy']].groupby('bookingID').count().cumsum().values.ravel()\n    bking_index_details = dict((k, (start, end)) for k, start, end in \\\n                              zip(bking_index, bking_index_start, bking_index_end))\n    \n    return bking_index_details, len(bking_index)\n\ndef get_rotation_matrix_3d(i_v, unit=None):\n    \n    \"\"\"\n    Obtain the 3d rotation matrix that rotates the vector to maximise its magnitude in a single axis. This is used to re-orientate the phone's coordinate system to the absolute coordinate system of the vehicle.\n    \n    Code reference : https://stackoverflow.com/questions/43507491/imprecision-with-rotation-matrix-to-align-a-vector-to-an-axis\n    \"\"\"\n    \n    if unit is None:\n        unit = [1.0, 0.0, 0.0]\n    # Normalize vector length\n    i_v /= np.linalg.norm(i_v)\n\n    # Get axis\n    uvw = np.cross(i_v, unit)\n\n    # compute trig values - no need to go through arccos and back\n    rcos = np.dot(i_v, unit)\n    rsin = np.linalg.norm(uvw)\n\n    #normalize and unpack axis\n    if not np.isclose(rsin, 0):\n        uvw /= rsin\n    u, v, w = uvw\n\n    # Compute rotation matrix - re-expressed to show structure\n    return (\n        rcos * np.eye(3) +\n        rsin * np.array([\n            [ 0, -w,  v],\n            [ w,  0, -u],\n            [-v,  u,  0]\n        ]) +\n        (1.0 - rcos) * uvw[:,None] * uvw[None,:]\n    )\n\ndef get_df_3d_matrix_g_multiplier(dataframe, num_booking):\n\n    \"\"\"\n    Obtain the 3d rotation matrix for each booking, and the g multiplier needed to restore the z-axis magnitude to 9.81 (gravitational force)\n    \"\"\"\n    \n    acc_medians = dataframe[['acceleration_x',\\\n                             'acceleration_y',\\\n                             'acceleration_z']].groupby(by='bookingID').median()\n    \n    acc_medians['acc_mag'] = acc_medians.apply(lambda x : np.sqrt(x['acceleration_x'] ** 2 + \\\n                                                                  x['acceleration_y'] ** 2 + \\\n                                                                  x['acceleration_z'] ** 2), axis=1)\n    \n    g_multiplier = acc_medians['acc_mag'].values / 9.81\n    \n    acc_medians_values = acc_medians[['acceleration_x',\\\n                                      'acceleration_y',\\\n                                      'acceleration_z']].values\n    rotation_matrix_3d = np.zeros((num_booking, 3, 3))\n    for i, acc_val in enumerate(tqdm(acc_medians_values)):\n        rotation_matrix_3d[i] = get_rotation_matrix_3d(acc_val)\n    \n    return rotation_matrix_3d, g_multiplier\n\ndef correct_xyz(dataframe, bking_details, rotation_matrix_3d, g_multiplier):\n\n    \"\"\"\n    Apply the rotation matrix to the bookingID's xyz accelerations entry, on the assumption that the phone orientation within the vehicle remains constant throughout the journey.\n    \"\"\"\n    \n    acc_original = dataframe[['acceleration_x', 'acceleration_y','acceleration_z']].values\n    acc_corrected = np.zeros_like(acc_original)\n\n    for b_index, (rot, g, (start, end)) in enumerate(tqdm(zip(rotation_matrix_3d, g_multiplier, bking_details.values()), total=len(g_multiplier))):\n        for r_index, row in enumerate(acc_original[start:end], start):\n            acc_corrected[r_index] = np.dot(row.T, rot.T) * g\n\n    dataframe[['acceleration_z', 'acceleration_x','acceleration_y']] = acc_corrected\n    \n    return dataframe\n\ndef shift_start_time(dataframe, bking_details):\n\n    \"\"\"\n    Shift the time with respect to the earliest timestamp.\n    \"\"\"\n    bkingid_start_index = [start for start, _ in bking_details.values()]\n    \n    dataframe.reset_index(inplace=True)\n    zerolised_time = dataframe['second'].values\n    for start, end in tqdm(bking_details.values()):\n        bking_start_time = dataframe.iloc[start, dataframe.columns.get_loc('second')]\n        zerolised_time[start : end] -= bking_start_time\n    dataframe['second'] = zerolised_time\n    \n    dataframe.drop(labels = dataframe.loc[dataframe['second'] >= 1800,:].index, inplace=True)\n    \n    dataframe.set_index(['bookingID','second'], inplace=True)\n    \n    return dataframe\n\ndef interpolate_time(dataframe, bking_details):\n    \n    \"\"\"\n    Interpolate missing time and features in between entries.\n    \"\"\"\n    \n    dataframe.reset_index(inplace=True)\n    \n    bking_interpolate_list = dict()\n    \n    for bkingid, (start, end) in tqdm(bking_details.items()):\n        unique_time = set(dataframe.iloc[start:end, dataframe.columns.get_loc('second')])\n        full_time_list = set(range(int(min(unique_time)), int(max(unique_time)) + 1, 1))\n        bking_interpolate_list[bkingid] = full_time_list.difference(unique_time)\n    \n    bking_interpolate_len = dict((k, len(v)) for k, v in bking_interpolate_list.items())\n    \n    row_num = sum(bking_interpolate_len.values())\n    col_num = len(dataframe.columns)\n    \n    interpolate_arr = np.empty((row_num, col_num))\n    interpolate_arr[:] = np.nan\n    \n    count = 0\n    \n    for bkingid, missing_time in tqdm(bking_interpolate_list.items()):\n        for time in missing_time:\n            interpolate_arr[count][dataframe.columns.get_loc('bookingID')] = bkingid\n            interpolate_arr[count][dataframe.columns.get_loc('second')] = time\n            count += 1\n    \n    interpolate_df = pd.DataFrame(interpolate_arr, columns=dataframe.columns)\n    \n    dataframe = pd.concat([dataframe, interpolate_df])\n    dataframe['bookingID'] = dataframe['bookingID'].astype('int64')\n    dataframe.sort_values(by=['bookingID','second'], inplace=True)\n    dataframe.set_index(['bookingID','second'], inplace=True)\n    \n    dataframe.interpolate(inplace=True)\n    \n    return dataframe\n    \ndef get_engineered_features(dataframe, bking_details):\n    \n    \"\"\"\n    Correct speed more than 35 m/s, convert bearing to radians, and generate desired features:\n    1. Acceleration -> Derivative of Speed\n    2. Turning Aggression -> log(| AngularFrequency * Speed |)\n    3. XY Acceleration Magnitude -> sqrt(AccelerationX ** 2 + AccelerationY **2)\n    \"\"\"\n    \n    start_idx = [x[0] for x in bking_details.values()]\n    \n    # remove speed outliers (more than 35 m/s is unlikely in Singapore)\n    dataframe.loc[dataframe['Speed'] > 35, 'Speed']  = 35\n    \n    dataframe['Acc_derived'] = (dataframe['Speed'] - dataframe['Speed'].shift(1))\n    dataframe.iloc[start_idx, dataframe.columns.get_loc('Acc_derived')] = 0\n\n    #dataframe['Jerk_derived'] = (dataframe['Acc_derived'] - dataframe['Acc_derived'].shift(1))\n    #dataframe.iloc[start_idx, dataframe.columns.get_loc('Jerk_derived')] = 0\n\n    # change bearing to radians\n    dataframe['Bearing'] = dataframe['Bearing'].apply(lambda x : np.deg2rad(x))\n    dataframe['Angular_freq_derived'] = dataframe['Bearing'] - dataframe['Bearing'].shift(1)\n    dataframe.iloc[start_idx, dataframe.columns.get_loc('Angular_freq_derived')] = 0\n        \n    # get turning aggression - ang freq * speed\n    dataframe['Turning_aggression'] = np.log1p(np.abs(dataframe['Angular_freq_derived'] * dataframe['Speed']))\n    \n    dataframe['acc_xy_mag'] = np.sqrt(dataframe['acceleration_x'] ** 2 + dataframe['acceleration_y'] ** 2)\n\n    dataframe.drop(columns=['Bearing','Angular_freq_derived','acceleration_x','acceleration_y'], inplace=True)\n    \n    return dataframe\n\ndef get_moving_stat(dataframe, cols, window, bking_details, stat):\n    \n    \"\"\"\n    Generate the moving statistics of the selected columns.\n    \"\"\"\n    \n    for col in cols:\n        if col not in dataframe.columns:\n            print(f'{col} not in dataframe, please check')\n            return None\n    \n    new_cols = [col + f'_mvg_{stat}_{window}' for col in cols]\n    \n    for col in new_cols:\n        dataframe[col] = 0\n    \n    if stat == 'mean':\n        new_cols_values = dataframe[cols].rolling(window = window).mean().values\n    elif stat == 'max':\n        new_cols_values = dataframe[cols].rolling(window = window).max().values\n        \n    for start, _ in tqdm(bking_details.values()):\n        new_cols_values[start : start + window] = 0\n    \n    dataframe[new_cols] = new_cols_values\n    \n    return dataframe\n\ndef generate_feature_statistics(dataframe):\n    \n    \"\"\"\n    Generate feature statistics of the processed features set.\n    \"\"\"\n    \n    features_medians = dataframe.groupby('bookingID').median()\n    features_max = dataframe.groupby('bookingID').max()\n    features_mean = dataframe.groupby('bookingID').mean()\n    features_skew = dataframe.groupby('bookingID').skew()\n    features_kurt = dataframe.groupby('bookingID').apply(pd.DataFrame.kurt)\n    \n    features_medians.columns = [x + '_median' for x in features_medians.columns]\n    features_max.columns = [x + '_max' for x in features_max.columns]\n    features_mean.columns = [x + '_mean' for x in features_mean.columns]\n    features_skew.columns = [x + '_skew' for x in features_skew.columns]\n    features_kurt.columns = [x + '_kurt' for x in features_kurt.columns]\n    \n    feature_stats = pd.concat([features_medians, features_max, features_mean, features_skew, features_kurt], axis=1)\n    \n    values = feature_stats.values\n    values[np.isnan(values)] = 0\n    \n    feature_stats[feature_stats.columns] = values\n    \n    return feature_stats\n   \ndef parallelize_dataframe(df_split, func, num_workers):\n    pool = Pool(num_workers)\n    df = pd.concat(pool.map(func, df_split))\n    pool.close()\n    pool.join()\n    return df\n\ndef get_all_features(features):\n    \n    \"\"\"\n    Obtain various features and update the features dataframe\n    \"\"\"\n    \n    bking_index_details, num_bkings = get_booking_details(features)\n\n    # correct xyz coordinates\n    rot3d, gmul = get_df_3d_matrix_g_multiplier(features, num_bkings)    \n    features = correct_xyz(features, bking_index_details, rot3d, gmul)\n    \n    # shift and interpolate\n    features = shift_start_time(features, bking_index_details)\n    bking_index_details, num_bkings = get_booking_details(features)\n    features = interpolate_time(features, bking_index_details)\n    bking_index_details, num_bkings = get_booking_details(features)\n    \n    # get engineered features\n    features = get_engineered_features(features, bking_index_details)\n    \n    # get moving stats\n    cols_for_mvg_stat = ['acceleration_z', 'Speed', 'Acc_derived', 'acc_xy_mag']\n    features = get_moving_stat(features, cols_for_mvg_stat, 10, bking_index_details, 'mean')\n    features = get_moving_stat(features, cols_for_mvg_stat, 10, bking_index_details, 'max')\n    cols_for_mvg_stat = ['Turning_aggression']\n    features = get_moving_stat(features, cols_for_mvg_stat, 3, bking_index_details, 'mean')\n    \n    features.drop(columns=['Turning_aggression','acc_xy_mag','acc_xy_mag_mvg_mean_10'], inplace=True)\n    if not (~features.isna()).all().all():\n        features.interpolate(inplace=True)\n        \n    return features\n\ndef get_dataframe_partitions(features, num_partition):\n    \n    \"\"\"\n    Partition dataframe into smaller partitions, to enable multiprocessing\n    \"\"\"\n    \n    bking_index_details, num_bkings = get_booking_details(features)\n    bookingIDs = np.array(features.index.get_level_values(0).unique())\n    partition = []\n    partition_size = int(bookingIDs.shape[0] / num_partition)\n    for i in range(num_partition):\n        start_id =  bookingIDs[partition_size * i]\n        if i + 1 == num_partition:\n            # to cater for edge cases\n            end_id = bookingIDs[-1]\n        else:\n            end_id = bookingIDs[partition_size * (i+1) - 1]\n        partition.append((bking_index_details[start_id][0], \n                        bking_index_details[end_id][1]))  \n    features_partitioned = [features.iloc[start:end,:] for start,end in partition]\n    return features_partitioned\n\ndef preprocess_and_save_data():\n\n    \"\"\"\n    Preprocess data and save the processed features as .h5 and features statistics as .csv\n    \"\"\"\n    \n    print('Step 1 / 3 : Loading all the .csv files.. This took around 1 min on the given dataset')\n    features = load_all_csv()\n    features = features.astype(np.float32)\n    \n    num_partition = cpu_count()\n    print('Step 2 / 3 : Generating features... This took around 2 mins on 4 workers for the given dataset.')\n    print(f'Parallelizing jobs, using {num_partition} workers')\n    features_partitioned = get_dataframe_partitions(features, num_partition)\n    features = parallelize_dataframe(features_partitioned, get_all_features, num_partition)\n    \n    features_partitioned = get_dataframe_partitions(features, num_partition)\n    feature_statistics = parallelize_dataframe(features_partitioned, generate_feature_statistics, num_partition)\n\n    print('Step 3 / 3 : Saving features and features_stats')\n    features.to_hdf('preprocessed_features.h5', key='features')\n    \n    feature_statistics.to_hdf('feature_stats.h5', key='feature_stats')\n    \n    bookingIDs = np.array(feature_statistics.index)\n    \n    return bookingIDs","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntqdm.pandas()\n_ = preprocess_and_save_data()","execution_count":2,"outputs":[{"output_type":"stream","text":"Step 1 / 3 : Loading all the .csv files.. This took around 1 min on the given dataset\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 9/9 [00:38<00:00,  4.33s/it]\n","name":"stderr"},{"output_type":"stream","text":"Step 2 / 3 : Generating features... This took around 2 mins on 4 workers for the given dataset.\nParallelizing jobs, using 4 workers\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 5000/5000 [00:01<00:00, 4630.88it/s]\n100%|██████████| 5000/5000 [00:01<00:00, 3476.14it/s]\n100%|██████████| 5000/5000 [00:01<00:00, 3816.26it/s]\n100%|██████████| 5000/5000 [00:01<00:00, 3821.69it/s]\n100%|██████████| 5000/5000 [00:29<00:00, 167.87it/s]\n100%|██████████| 5000/5000 [00:00<00:00, 28083.01it/s]\n100%|██████████| 5000/5000 [00:30<00:00, 164.45it/s]\n100%|██████████| 5000/5000 [00:00<00:00, 31183.30it/s]\n100%|██████████| 5000/5000 [00:30<00:00, 161.97it/s]\n100%|██████████| 5000/5000 [00:00<00:00, 30156.10it/s]\n100%|██████████| 5000/5000 [00:30<00:00, 163.74it/s]\n100%|██████████| 5000/5000 [00:00<00:00, 26011.28it/s]\n100%|██████████| 5000/5000 [00:03<00:00, 1436.08it/s]\n100%|██████████| 5000/5000 [00:03<00:00, 1506.16it/s]\n100%|██████████| 5000/5000 [00:03<00:00, 1467.26it/s]\n100%|██████████| 5000/5000 [00:03<00:00, 1450.59it/s]\n100%|██████████| 5000/5000 [00:03<00:00, 1577.09it/s]\n100%|██████████| 5000/5000 [00:03<00:00, 1509.81it/s]\n100%|██████████| 5000/5000 [00:03<00:00, 1486.73it/s]\n100%|██████████| 5000/5000 [00:03<00:00, 1624.71it/s]\n100%|██████████| 5000/5000 [00:00<00:00, 242981.84it/s]\n100%|██████████| 5000/5000 [00:00<00:00, 259349.51it/s]\n100%|██████████| 5000/5000 [00:00<00:00, 293792.83it/s]\n100%|██████████| 5000/5000 [00:00<00:00, 259208.46it/s]\n100%|██████████| 5000/5000 [00:00<00:00, 267095.28it/s]\n100%|██████████| 5000/5000 [00:00<00:00, 299730.16it/s]\n100%|██████████| 5000/5000 [00:00<00:00, 294630.72it/s]\n100%|██████████| 5000/5000 [00:00<00:00, 297430.40it/s]\n100%|██████████| 5000/5000 [00:00<00:00, 329632.04it/s]\n100%|██████████| 5000/5000 [00:00<00:00, 321708.29it/s]\n100%|██████████| 5000/5000 [00:00<00:00, 340170.64it/s]\n100%|██████████| 5000/5000 [00:00<00:00, 333431.70it/s]\n","name":"stderr"},{"output_type":"stream","text":"Step 3 / 3 : Saving features and features_stats\nCPU times: user 1min 2s, sys: 30.2 s, total: 1min 32s\nWall time: 3min 13s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}